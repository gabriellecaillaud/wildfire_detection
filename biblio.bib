
@article{mannem_smart_2024,
	title = {Smart audio signal classification for tracking of construction tasks},
	volume = {165},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580524002218},
	doi = {https://doi.org/10.1016/j.autcon.2024.105485},
	abstract = {This paper presents a model for sound classification in construction that leverages a unique combination of Mel spectrograms and Mel-Frequency Cepstral Coefficient ({MFCC}) values. This model combines deep neural networks like Convolution Neural Networks ({CNN}) and Long short-term memory ({LSTM}) to create {CNN}-{LSTM} and {MFCCs}-{LSTM} architectures, enabling the extraction of spectral and temporal features from audio data. The audio data, generated from construction activities in a real-time closed environment is used to evaluate the proposed model and resulted in an overall Precision, Recall, and F1-score of 91\%, 89\%, and 91\%, respectively. This performance surpasses other established models, including Deep Neural Networks ({DNN}), {CNN}, and Recurrent Neural Networks ({RNN}), as well as a combination of these models as {CNN}-{DNN}, {CNN}-{RNN}, and {CNN}-{LSTM}. These results underscore the potential of combining Mel spectrograms and {MFCC} values to provide a more informative representation of sound data, thereby enhancing sound classification in noisy environments.},
	pages = {105485},
	journaltitle = {Automation in Construction},
	author = {Mannem, Karunakar Reddy and Mengiste, Eyob and Hasan, Saed and Soto, Borja García de and Sacks, Rafael},
	date = {2024},
	keywords = {Activity tracking, Audio, {CNN}, {LSTM}, Mel spectrograms, {MFCC}, Sound},
}

@article{zhang_wildfire_2019,
	title = {Wildfire Detection Using Sound Spectrum Analysis Based on the Internet of Things},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/23/5093},
	doi = {10.3390/s19235093},
	abstract = {Wildfire is a sudden and hazardous natural disaster. Currently, many schemes based on optical spectrum analysis have been proposed to detect wildfire, but obstacles in forest areas can decrease the efficiency of spectral monitoring, resulting in a wildfire detection system not being able to monitor the occurrence of wildfire promptly. In this paper, we propose a novel wildfire detection system using sound spectrum analysis based on the Internet of Things ({IoT}), which utilizes a wireless acoustic detection system to probe wildfire and distinguish the difference in the sound between the crown and the surface fire. We also designed a new power supply unit: tree-energy device, which utilizes the biological energy of the living trees to generate electricity. We implemented sound spectrum analysis on the data collected by sound sensors and then combined our classification algorithms. The results describe that the sound frequency of the crown fire is about 0–400 Hz, while the sound frequency of the surface fire ranges from 0 to 15,000 Hz. However, the accuracy of the classification method is affected by some factors, such as the distribution of sensors, the loss of energy in sound transmission, and the delay of data transmission. In the simulation experiments, the recognition rate of the method can reach about 70\%.},
	number = {23},
	journaltitle = {Sensors},
	author = {Zhang, Shuo and Gao, Demin and Lin, Haifeng and Sun, Quan},
	date = {2019},
}

@article{huang_audio-based_2022,
	title = {Audio-Based Wildfire Detection on Embedded Systems},
	volume = {11},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/11/9/1417},
	doi = {10.3390/electronics11091417},
	abstract = {The occurrence of wildfires often results in significant fatalities. As wildfires are notorious for their high speed of spread, the ability to identify wildfire at its early stage is essential in quickly obtaining control of the fire and in reducing property loss and preventing loss of life. This work presents a machine learning wildfire detecting data pipeline that can be deployed on embedded systems in remote locations. The proposed data pipeline consists of three main steps: audio preprocessing, feature engineering, and classification. Experiments show that the proposed data pipeline is capable of detecting wildfire effectively with high precision and is capable of detecting wildfire sound over the forest’s background soundscape. When being deployed on a Raspberry Pi 4, the proposed data pipeline takes 66 milliseconds to process a 1 s sound clip. To the knowledge of the author, this is the first edge-computing implementation of an audio-based wildfire detection system.},
	number = {9},
	journaltitle = {Electronics},
	author = {Huang, Hung-Tien and Downey, Austin R. J. and Bakos, Jason D.},
	date = {2022},
}

@inproceedings{karunaratna_sound_2021,
	title = {Sound Events Recognition and Classification Using Machine Learning Techniques},
	author = {Karunaratna, Sulakna and Maduranga, Pasan},
	date = {2021-09},
}

@article{bansal_environmental_2023,
	title = {Environmental Sound Classification using Hybrid Ensemble Model},
	volume = {218},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050923000248},
	doi = {https://doi.org/10.1016/j.procs.2023.01.024},
	abstract = {Environmental sound classification ({ESC}) is the most trending research areas. The sounds in the surroundings such as screaming, air conditioners, and rain droplets can help in the development of context-aware applications. It is complex to process the envi- ronmental sounds as compared to speech and music due to the unstructured essence of environmental sounds. In the past, certain preprocessing techniques, feature extraction, and classification algorithms are used for {ESC}. Several researchers have applied ma- chine learning classifiers for {ESC} and certain ensemble classifiers are also used but the accuracy can be increased if instead of combining homogeneous classifiers, heterogeneous classifiers can be ensembled. In this paper, a hybrid ensemble classifier is used for {ESC} on the {UrbanSound}8k dataset and cepstral features Mel Frequency Cepstral Coefficients are used. Five different machine learning classifiers- Decision Tree, Support Vector Machine, Logistic Regression, K- Nearest Neighbour, and Naive Bayes are used to develop a hybrid ensemble model. The highest accuracy is obtained when all the five classifiers are combined. The proposed approach gives an accuracy of 79.4\% and is compared with the benchmark results using individual classifiers and the former out- performs the latter. The results of the hybrid ensemble model on the {UrbanSound}8K dataset are also compared with the dataset {ESC}-10.},
	pages = {418--428},
	journaltitle = {Procedia Computer Science},
	author = {Bansal, Anam and Garg, Naresh Kumar},
	date = {2023},
	keywords = {Feature extraction, Environmental Sound Classification, Hybrid ensemble, Machine learning},
}

@article{grari_forest_2023,
	title = {{FOREST} {FIRE} {DETECTION} {AND} {MONITORING} {THROUGH} {ENVIRONMENT} {SOUND} {SPECTRUM} {USING} {DEEP} {LEARNING}},
	author = {Grari, Mounir and Boukabous, Mohammed and Yandouzi, Mimoun and Mohammed, Berrahal and Idrissi, Idriss and Moussaoui, Omar and Azizi, Mostafa and Moussaoui, Mimoun},
	date = {2023-10},
}

@article{peruzzi_fight_2023,
	title = {Fight Fire with Fire: Detecting Forest Fires with Embedded Machine Learning Models Dealing with Audio and Images on Low Power {IoT} Devices},
	volume = {23},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/23/2/783},
	doi = {10.3390/s23020783},
	abstract = {Forest fires are the main cause of desertification, and they have a disastrous impact on agricultural and forest ecosystems. Modern fire detection and warning systems rely on several techniques: satellite monitoring, sensor networks, image processing, data fusion, etc. Recently, Artificial Intelligence ({AI}) algorithms have been applied to fire recognition systems, enhancing their efficiency and reliability. However, these devices usually need constant data transmission along with a proper amount of computing power, entailing high costs and energy consumption. This paper presents the prototype of a Video Surveillance Unit ({VSU}) for recognising and signalling the presence of forest fires by exploiting two embedded Machine Learning ({ML}) algorithms running on a low power device. The {ML} models take audio samples and images as their respective inputs, allowing for timely fire detection. The main result is that while the performances of the two models are comparable when they work independently, their joint usage according to the proposed methodology provides a higher accuracy, precision, recall and F1 score (96.15\%, 92.30\%, 100.00\%, and 96.00\%, respectively). Eventually, each event is remotely signalled by making use of the Long Range Wide Area Network ({LoRaWAN}) protocol to ensure that the personnel in charge are able to operate promptly.},
	number = {2},
	journaltitle = {Sensors},
	author = {Peruzzi, Giacomo and Pozzebon, Alessandro and Van Der Meer, Mattia},
	date = {2023},
}

@article{ullo_hybrid_2020,
	title = {Hybrid Computerized Method for Environmental Sound Classification},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3006082},
	abstract = {Classification of environmental sounds plays a key role in security, investigation, robotics since the study of the sounds present in a specific environment can allow to get significant insights. Lack of standardized methods for an automatic and effective environmental sound classification ({ESC}) creates a need to be urgently satisfied. As a response to this limitation, in this paper, a hybrid model for automatic and accurate classification of environmental sounds is proposed. Optimum allocation sampling ({OAS}) is used to elicit the informative samples from each class. The representative samples obtained by {OAS} are turned into the spectrogram containing their time-frequency-amplitude representation by using a short-time Fourier transform ({STFT}). The spectrogram is then given as an input to pre-trained {AlexNet} and Visual Geometry Group ({VGG})-16 networks. Multiple deep features are extracted using the pre-trained networks and classified by using multiple classification techniques namely decision tree (fine, medium, coarse kernel), k-nearest neighbor (fine, medium, cosine, cubic, coarse and weighted kernel), support vector machine, linear discriminant analysis, bagged tree and softmax classifiers. The {ESC}-10, a ten-class environmental sound dataset, is used for the evaluation of the methodology. An accuracy of 90.1\%, 95.8\%, 94.7\%, 87.9\%, 95.6\%, and 92.4\% is obtained with a decision tree, k-neared neighbor, support vector machine, linear discriminant analysis, bagged tree and softmax classifier respectively. The proposed method proved to be robust, effective, and promising in comparison with other existing state-of-the-art techniques, using the same dataset.},
	pages = {124055--124065},
	journaltitle = {{IEEE} Access},
	author = {Ullo, Silvia Liberata and Khare, Smith K. and Bajaj, Varun and Sinha, G. R.},
	date = {2020},
	keywords = {Feature extraction, Spectrogram, classification techniques, convolutional neural network, Environmental sound classification, Fourier transforms, Hidden Markov models, Microsoft Windows, optimal allocation sampling, Resource management, spectrogram, Support vector machines},
}

@article{yang_preferred_2023,
	title = {Preferred vector machine for forest fire detection},
	volume = {143},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S003132032300420X},
	doi = {https://doi.org/10.1016/j.patcog.2023.109722},
	abstract = {Machine learning-based fire detection/recognition is very popular in forest-monitoring systems. However, without considering the prior knowledge, e.g., equal attention on both classes of the fire and non-fire samples, fire miss-detected phenomena frequently appeared in the current methods. In this work, considering model’s interpretability and the limited data for model-training, we propose a novel pixel-precision method, termed as {PreVM} (Preferred Vector Machine). To guarantee high fire detection rate under precise control, a new L0 norm constraint is introduced to the fire class. Computationally, instead of the traditional L1 re-weighted techniques in L0 norm approximation, this L0 constraint can be converted into linear inequality and incorporated into the process of parameter selection. To further speed up model-training and reduce error warning rate, we also present a kernel-based L1 norm {PreVM} (L1-{PreVM}). Theoretically, we firstly prove the existence of dual representation for the general Lp (p≥1) norm regularization problems in {RKHS} (Reproducing Kernel Hilbert Space). Then, we provide a mathematical evidence for L1 norm kernelization to conquer the case when feature samples do not appear in pairs. The work also includes an extensive experimentation on the real forest fire images and videos. Compared with the-state-of-art methods, the results show that our {PreVM} is capable of simultaneously achieving higher fire detection rates and lower error warning rates, and L1-{PreVM} is also superior in real-time detection.},
	pages = {109722},
	journaltitle = {Pattern Recognition},
	author = {Yang, Xubing and Hua, Zhichun and Zhang, Li and Fan, Xijian and Zhang, Fuquan and Ye, Qiaolin and Fu, Liyong},
	date = {2023},
	keywords = {Forest fire detection, Dual representation, Error warning rate, Fire detection rate, {SVM}},
}

@article{bhadoria_rvfr_2021,
	title = {{RVFR}: Random vector forest regression model for integrated \& enhanced approach in forest fires predictions},
	volume = {66},
	issn = {1574-9541},
	url = {https://www.sciencedirect.com/science/article/pii/S1574954121002624},
	doi = {https://doi.org/10.1016/j.ecoinf.2021.101471},
	abstract = {The forest fires is one of the most dangerous disasters to the livelihood planet of earth. Human intervention into the field of the destruction of nature is another cause of these forest fires. The ability to heal nature is dampened due to the expansion of human territories into forests causing loss of lands for the forests. While human expansion cannot be halted, so we as human must take a responsibility for the consequences and make sure the climatic changes due to such disasters, should be as low as possible. Halting the forest fires is an impossible task, and the best we can do control the fires and the amount of area burnt. This process can be improved by predicting the forest fires and the amount of area that might get burnt due to the predicted fire and take measures against it such that there would be no fire at all or the very least reduce the amount of area burnt during the fire. Machine learning in predictions of forest fires is heavily researched and mostly used in real-time. Though usage of machine learning algorithms has been in use, the accuracy of these models is of the lower levels. The paper introduces using a model which is a hybrid of its predecessors, taking as many postulates from them as possible and leaving the cons behind. The hybrid model which is a combination of support vector machine and random forest regression model is using the data of forest fires in the period of 2011–2020 in India, and able to give more accurate predictions. The proposed {RVFR} model also results in achieving an accuracy of 94\%, which is greater than all of the predecessors and with a variance of 1.0.},
	pages = {101471},
	journaltitle = {Ecological Informatics},
	author = {Bhadoria, Robin Singh and Pandey, Manish Kumar and Kundu, Pradeep},
	date = {2021},
	keywords = {Machine learning, Forest fires prediction, Random vector forest regression},
}

@article{sun_novel_2024,
	title = {A novel study for depression detecting using audio signals based on graph neural network},
	volume = {88},
	issn = {1746-8094},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809423011084},
	doi = {https://doi.org/10.1016/j.bspc.2023.105675},
	abstract = {Depression is a prevalent mental health disorder. The absence of specific biomarkers makes clinical diagnosis highly subjective. This makes it difficult to make a definitive diagnosis for the patient. Recently, deep learning methods have shown promise for depression detection. However, current methods tend to focus solely on the connections within or between audio signals, leading to limitations in the model’s ability to recognize depression-related cues in audio signals and affecting its classification performance. To address these limitations, we propose a graph neural network approach for depression recognition that incorporates potential connections within and between audio signals. Specifically, we first use a gated recurrent unit ({GRU}) to extract time-series information between frame-level features of audio signals. We then construct two graph neural network modules sequentially to explore the potential connections within and between audio signals. The first graph network module constructs a graph using the frame-level features of each audio sample as nodes. The output is obtained as a graph-embedded feature vector representation after the graph convolution layers. Subsequently, the output graph embedding feature vector representation of the first graph network model is used as the nodes of the graph to construct the second graph network. The internal relationship between audio signals is encoded by the property of node neighborhood information propagation. In addition, we use a pre-trained emotion recognition network to extract emotional features that are highly correlated with depression. By further strengthening the connection weights among nodes in the second graph network through a self-attention mechanism, relevant cues are provided for the model to complete depression detection from audio signals. We conducted extensive experiments on three depression datasets, including {DAIC}-{WOZ}, {MODMA}, and D-Vlog. The proposed model achieves better results on several performance evaluation metrics such as accuracy, F1-score, precision, and recall compared to all the compared algorithms, validating its effectiveness.},
	pages = {105675},
	journaltitle = {Biomedical Signal Processing and Control},
	author = {Sun, Chenjian and Jiang, Min and Gao, Linlin and Xin, Yu and Dong, Yihong},
	date = {2024},
	keywords = {Audio recognition, Automatic depression detection, Graph neural network},
}

@article{mannem_smart_2024-1,
	title = {Smart audio signal classification for tracking of construction tasks},
	volume = {165},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580524002218},
	doi = {https://doi.org/10.1016/j.autcon.2024.105485},
	abstract = {This paper presents a model for sound classification in construction that leverages a unique combination of Mel spectrograms and Mel-Frequency Cepstral Coefficient ({MFCC}) values. This model combines deep neural networks like Convolution Neural Networks ({CNN}) and Long short-term memory ({LSTM}) to create {CNN}-{LSTM} and {MFCCs}-{LSTM} architectures, enabling the extraction of spectral and temporal features from audio data. The audio data, generated from construction activities in a real-time closed environment is used to evaluate the proposed model and resulted in an overall Precision, Recall, and F1-score of 91\%, 89\%, and 91\%, respectively. This performance surpasses other established models, including Deep Neural Networks ({DNN}), {CNN}, and Recurrent Neural Networks ({RNN}), as well as a combination of these models as {CNN}-{DNN}, {CNN}-{RNN}, and {CNN}-{LSTM}. These results underscore the potential of combining Mel spectrograms and {MFCC} values to provide a more informative representation of sound data, thereby enhancing sound classification in noisy environments.},
	pages = {105485},
	journaltitle = {Automation in Construction},
	author = {Mannem, Karunakar Reddy and Mengiste, Eyob and Hasan, Saed and Soto, Borja García de and Sacks, Rafael},
	date = {2024},
	keywords = {Activity tracking, Audio, {CNN}, {LSTM}, Mel spectrograms, {MFCC}, Sound},
}

@article{zaman_survey_2023,
	title = {A Survey of Audio Classification Using Deep Learning},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3318015},
	abstract = {Deep learning can be used for audio signal classification in a variety of ways. It can be used to detect and classify various types of audio signals such as speech, music, and environmental sounds. Deep learning models are able to learn complex patterns of audio signals and can be trained on large datasets to achieve high accuracy. To employ deep learning for audio signal classification, the audio signal must first be represented in a suitable form. This can be done using signal representation techniques such as using spectrograms, Mel-frequency Cepstral coefficients, linear predictive coding, and wavelet decomposition. Once the audio signal is represented in a suitable form, it can then be fed into a deep learning model. Various deep learning models can be utilized for audio classification. We provide an extensive survey of current deep learning models used for a variety of audio classification tasks. In particular, we focus on works published under five different deep neural network architectures, namely Convolutional Neural Networks ({CNNs}), Recurrent Neural Networks ({RNNs}), Autoencoders, Transformers and Hybrid Models (hybrid deep learning models and hybrid deep learning models with traditional classifiers). {CNNs} can be used to classify audio signals into different categories such as speech, music, and environmental sounds. They can also be used for speech recognition, speaker identification, and emotion recognition. {RNNs} are widely used for audio classification and audio segmentation. {RNN} models can capture temporal patterns of audio signals and be used to classify audio segments into different categories. Another approach is to use autoencoders for learning the features of audio signals and then classifying the signals into different categories. Transformers are also well-suited for audio classification. In particular, temporal and frequency features can be extracted to identify the characteristics of the audio signals. Finally, hybrid models for audio classification either combine various deep learning architectures (i.e. {CNN}-{RNN}) or combine deep learning models with traditional machine learning techniques (i.e. {CNN}-Support Vector Machine). These hybrid models take advantage of the strengths of different architectures while avoiding their weaknesses. Existing literature under different categories of deep learning are summarized and compared in detail.},
	pages = {106620--106649},
	journaltitle = {{IEEE} Access},
	author = {Zaman, Khalid and Sah, Melike and Direkoglu, Cem and Unoki, Masashi},
	date = {2023},
	keywords = {Audio, Feature extraction, Task analysis, Hidden Markov models, Audio systems, autoencoders, classification, Classification algorithms, {CNNs}, Data models, deep learning, Deep learning, emotion, Emotion recognition, hybrid models, music, noise, recognition, Recurrent neural networks, {RNNs}, speech, Speech recognition, Surveys, transformers},
}

@inproceedings{gong_whisper-at_2023,
	title = {Whisper-{AT}: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers},
	url = {http://arxiv.org/abs/2307.03183},
	doi = {10.21437/Interspeech.2023-2193},
	shorttitle = {Whisper-{AT}},
	abstract = {In this paper, we focus on Whisper, a recent automatic speech recognition model trained with a massive 680k hour labeled speech corpus recorded in diverse conditions. We first show an interesting finding that while Whisper is very robust against real-world background sounds (e.g., music), its audio representation is actually not noise-invariant, but is instead highly correlated to non-speech sounds, indicating that Whisper recognizes speech conditioned on the noise type. With this finding, we build a unified audio tagging and speech recognition model Whisper-{AT} by freezing the backbone of Whisper, and training a lightweight audio tagging model on top of it. With {\textless}1\% extra computational cost, Whisper-{AT} can recognize audio events, in addition to spoken text, in a single forward pass.},
	pages = {2798--2802},
	booktitle = {{INTERSPEECH} 2023},
	author = {Gong, Yuan and Khurana, Sameer and Karlinsky, Leonid and Glass, James},
	urldate = {2024-09-06},
	date = {2023-08-20},
	eprinttype = {arxiv},
	eprint = {2307.03183 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/adil/Zotero/storage/DZVSBICA/Gong et al. - 2023 - Whisper-AT Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers.pdf:application/pdf;arXiv.org Snapshot:/Users/adil/Zotero/storage/ZCUM7N88/2307.html:text/html},
}

@misc{radford_robust_2022,
	title = {Robust Speech Recognition via Large-Scale Weak Supervision},
	url = {http://arxiv.org/abs/2212.04356},
	abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
	number = {{arXiv}:2212.04356},
	publisher = {{arXiv}},
	author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and {McLeavey}, Christine and Sutskever, Ilya},
	urldate = {2024-09-06},
	date = {2022-12-06},
	eprinttype = {arxiv},
	eprint = {2212.04356 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/adil/Zotero/storage/8EHHBP6R/Radford et al. - 2022 - Robust Speech Recognition via Large-Scale Weak Supervision.pdf:application/pdf;arXiv.org Snapshot:/Users/adil/Zotero/storage/XJNQ5UQM/2212.html:text/html},
}

@article{mesaros_sound_2021,
	title = {Sound Event Detection: A Tutorial},
	volume = {38},
	issn = {1053-5888, 1558-0792},
	url = {http://arxiv.org/abs/2107.05463},
	doi = {10.1109/MSP.2021.3090678},
	shorttitle = {Sound Event Detection},
	abstract = {The goal of automatic sound event detection ({SED}) methods is to recognize what is happening in an audio signal and when it is happening. In practice, the goal is to recognize at what temporal instances different sounds are active within an audio signal. This paper gives a tutorial presentation of sound event detection, including its definition, signal processing and machine learning approaches, evaluation, and future perspectives.},
	pages = {67--83},
	number = {5},
	journaltitle = {{IEEE} Signal Processing Magazine},
	shortjournal = {{IEEE} Signal Process. Mag.},
	author = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas and Plumbley, Mark D.},
	urldate = {2024-09-06},
	date = {2021-09},
	eprinttype = {arxiv},
	eprint = {2107.05463 [eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/adil/Zotero/storage/PCZ7MG92/Mesaros et al. - 2021 - Sound Event Detection A Tutorial.pdf:application/pdf;arXiv.org Snapshot:/Users/adil/Zotero/storage/GSJVBXDB/2107.html:text/html},
}

@misc{gong_ast_2021,
	title = {{AST}: Audio Spectrogram Transformer},
	url = {http://arxiv.org/abs/2104.01778},
	shorttitle = {{AST}},
	abstract = {In the past decade, convolutional neural networks ({CNNs}) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the {CNN}, forming a {CNN}-attention hybrid model. However, it is unclear whether the reliance on a {CNN} is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer ({AST}), the first convolution-free, purely attention-based model for audio classification. We evaluate {AST} on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 {mAP} on {AudioSet}, 95.6\% accuracy on {ESC}-50, and 98.1\% accuracy on Speech Commands V2.},
	number = {{arXiv}:2104.01778},
	publisher = {{arXiv}},
	author = {Gong, Yuan and Chung, Yu-An and Glass, James},
	urldate = {2024-09-06},
	date = {2021-07-08},
	eprinttype = {arxiv},
	eprint = {2104.01778 [cs]},
	keywords = {Computer Science - Sound, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/adil/Zotero/storage/UEPB6KWU/Gong et al. - 2021 - AST Audio Spectrogram Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/adil/Zotero/storage/MVTH3ITB/2104.html:text/html},
}

@misc{abdoli_end--end_2019,
	title = {End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network},
	url = {http://arxiv.org/abs/1904.08990},
	abstract = {In this paper, we present an end-to-end approach for environmental sound classification based on a 1D Convolution Neural Network ({CNN}) that learns a representation directly from the audio signal. Several convolutional layers are used to capture the signal's fine time structure and learn diverse filters that are relevant to the classification task. The proposed approach can deal with audio signals of any length as it splits the signal into overlapped frames using a sliding window. Different architectures considering several input sizes are evaluated, including the initialization of the first convolutional layer with a Gammatone filterbank that models the human auditory filter response in the cochlea. The performance of the proposed end-to-end approach in classifying environmental sounds was assessed on the {UrbanSound}8k dataset and the experimental results have shown that it achieves 89\% of mean accuracy. Therefore, the propose approach outperforms most of the state-of-the-art approaches that use handcrafted features or 2D representations as input. Furthermore, the proposed approach has a small number of parameters compared to other architectures found in the literature, which reduces the amount of data required for training.},
	number = {{arXiv}:1904.08990},
	publisher = {{arXiv}},
	author = {Abdoli, Sajjad and Cardinal, Patrick and Koerich, Alessandro Lameiras},
	urldate = {2024-09-06},
	date = {2019-04-18},
	eprinttype = {arxiv},
	eprint = {1904.08990 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/adil/Zotero/storage/CESRSKQT/Abdoli et al. - 2019 - End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/adil/Zotero/storage/599VZCPR/1904.html:text/html},
}
